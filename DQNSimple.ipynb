{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow_p27",
      "language": "python",
      "name": "conda_tensorflow_p27"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.17"
    },
    "colab": {
      "name": "DQNSimple.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenjiNakagawa/RL-public/blob/master/DQNSimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0wZ2fs1CPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "401384c7-a317-435b-99ce-9d51504fa706"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "# import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CatchBall:\n",
        "    def __init__(self):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Env\"\n",
        "        self.screen_n_rows = 8\n",
        "        self.screen_n_cols = 8\n",
        "        self.player_length = 3\n",
        "        self.enable_actions = (0, 1, 2)\n",
        "        self.frame_rate = 5\n",
        "\n",
        "        # variables\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, action):\n",
        "        \"\"\"\n",
        "        action:\n",
        "            0: do nothing\n",
        "            1: move left\n",
        "            2: move right\n",
        "        \"\"\"\n",
        "        # update player position\n",
        "        if action == self.enable_actions[1]:\n",
        "            # move left\n",
        "            self.player_col = max(0, self.player_col - 1)\n",
        "        elif action == self.enable_actions[2]:\n",
        "            # move right\n",
        "            self.player_col = min(self.player_col + 1, self.screen_n_cols - self.player_length)\n",
        "        else:\n",
        "            # do nothing\n",
        "            pass\n",
        "\n",
        "        # update ball position\n",
        "        self.ball_row += 1\n",
        "\n",
        "        # collision detection\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n",
        "        if self.ball_row == self.screen_n_rows - 1:\n",
        "            self.terminal = True\n",
        "            if self.player_col <= self.ball_col < self.player_col + self.player_length:\n",
        "                # catch\n",
        "                self.reward = 1\n",
        "            else:\n",
        "                # drop\n",
        "                self.reward = -1\n",
        "\n",
        "    def draw(self):\n",
        "        # reset screen\n",
        "        self.screen = np.zeros((self.screen_n_rows, self.screen_n_cols))\n",
        "\n",
        "        # draw player\n",
        "        self.screen[self.player_row, self.player_col:self.player_col + self.player_length] = 1\n",
        "\n",
        "        # draw ball\n",
        "        self.screen[self.ball_row, self.ball_col] = 1\n",
        "\n",
        "    def observe(self):\n",
        "        self.draw()\n",
        "        return self.screen, self.reward, self.terminal\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        self.update(action)\n",
        "\n",
        "    def reset(self):\n",
        "        # reset player position\n",
        "        self.player_row = self.screen_n_rows - 1\n",
        "        self.player_col = np.random.randint(self.screen_n_cols - self.player_length)\n",
        "\n",
        "        # reset ball position\n",
        "        self.ball_row = 0\n",
        "        self.ball_col = np.random.randint(self.screen_n_cols)\n",
        "\n",
        "        # reset other variables\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFfsW2g1CPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Multi Layer Perceptron with Experience Replay\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, enable_actions, environment_name):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Agent\"\n",
        "        self.environment_name = environment_name\n",
        "        self.enable_actions = enable_actions\n",
        "        self.n_actions = len(self.enable_actions)\n",
        "        self.minibatch_size = 32\n",
        "        self.replay_memory_size = 1000\n",
        "        self.learning_rate = 0.001\n",
        "        self.discount_factor = 0.9\n",
        "        self.exploration = 0.1\n",
        "        self.model_dir = \"models\"\n",
        "        self.model_name = \"{}.ckpt\".format(self.environment_name)\n",
        "\n",
        "        # replay memory\n",
        "        self.D = deque(maxlen=self.replay_memory_size)\n",
        "\n",
        "        # model\n",
        "        self.init_model()\n",
        "\n",
        "        # variables\n",
        "        self.current_loss = 0.0\n",
        "\n",
        "    def init_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "        # input layer (8 x 8)\n",
        "        self.x = tf.compat.v1.placeholder(tf.float32, [None, 8, 8])\n",
        "        \n",
        "        # flatten (64)\n",
        "        x_flat = tf.reshape(self.x, [-1, 64])\n",
        "\n",
        "        # fully connected layer (32)\n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([64, 64], stddev=0.01))\n",
        "        b_fc1 = tf.Variable(tf.zeros([64]))\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # output layer (n_actions)\n",
        "        W_out = tf.Variable(tf.truncated_normal([64, self.n_actions], stddev=0.01))\n",
        "        b_out = tf.Variable(tf.zeros([self.n_actions]))\n",
        "        self.y = tf.matmul(h_fc1, W_out) + b_out\n",
        "\n",
        "        # loss function\n",
        "        self.y_ = tf.compat.v1.placeholder(tf.float32, [None, self.n_actions])\n",
        "        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n",
        "\n",
        "        # train operation\n",
        "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
        "        self.training = optimizer.minimize(self.loss)\n",
        "\n",
        "        # saver\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def Q_values(self, state):\n",
        "        # Q(state, action) of all actions\n",
        "        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            # random\n",
        "            return np.random.choice(self.enable_actions)\n",
        "        else:\n",
        "            # max_action Q(state, action)\n",
        "            return self.enable_actions[np.argmax(self.Q_values(state))]\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_1, terminal):\n",
        "        self.D.append((state, action, reward, state_1, terminal))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        state_minibatch = []\n",
        "        y_minibatch = []\n",
        "\n",
        "        # sample random minibatch\n",
        "        minibatch_size = min(len(self.D), self.minibatch_size)\n",
        "        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n",
        "\n",
        "        for j in minibatch_indexes:\n",
        "            state_j, action_j, reward_j, state_j_1, terminal = self.D[j]\n",
        "            action_j_index = self.enable_actions.index(action_j)\n",
        "\n",
        "            y_j = self.Q_values(state_j)\n",
        "\n",
        "            if terminal:\n",
        "                y_j[action_j_index] = reward_j\n",
        "            else:\n",
        "                # reward_j + gamma * max_action' Q(state', action')\n",
        "                y_j[action_j_index] = reward_j + self.discount_factor * np.max(self.Q_values(state_j_1))  # NOQA\n",
        "\n",
        "            state_minibatch.append(state_j)\n",
        "            y_minibatch.append(y_j)\n",
        "\n",
        "        # training\n",
        "        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "        # for log\n",
        "        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        if model_path:\n",
        "            # load from model_path\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "        else:\n",
        "            # load from checkpoint\n",
        "            checkpoint = tf.train.get_checkpoint_state(self.model_dir)\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, os.path.join(self.model_dir, self.model_name))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-MMHxx1CPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "9295674b-d56b-4f37-9530-c464b6aca0d6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # parameters\n",
        "    n_epochs = 50\n",
        "\n",
        "    # environment, agent\n",
        "    env = CatchBall()\n",
        "    agent = DQNAgent(env.enable_actions, env.name)\n",
        "\n",
        "    # variables\n",
        "    win = 0\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # reset\n",
        "        frame = 0\n",
        "        loss = 0.0\n",
        "        Q_max = 0.0\n",
        "        env.reset()\n",
        "        state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "        while not terminal:\n",
        "            state_t = state_t_1\n",
        "\n",
        "            # execute action in environment\n",
        "            action_t = agent.select_action(state_t, agent.exploration)\n",
        "            env.execute_action(action_t)\n",
        "\n",
        "            # observe environment\n",
        "            state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "            # store experience\n",
        "            agent.store_experience(state_t, action_t, reward_t, state_t_1, terminal)\n",
        "\n",
        "            # experience replay\n",
        "            agent.experience_replay()\n",
        "\n",
        "            # for log\n",
        "            frame += 1\n",
        "            loss += agent.current_loss\n",
        "            Q_max += np.max(agent.Q_values(state_t))\n",
        "            if reward_t == 1:\n",
        "                win += 1\n",
        "\n",
        "        print(\"EPOCH: {:03d}/{:03d} | WIN: {:03d} | LOSS: {:.4f} | Q_MAX: {:.4f}\".format(\n",
        "            e, n_epochs - 1, win, loss / frame, Q_max / frame))\n",
        "\n",
        "    # save model\n",
        "    agent.save_model()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "EPOCH: 000/049 | WIN: 001 | LOSS: 0.0068 | Q_MAX: -0.0002\n",
            "EPOCH: 001/049 | WIN: 001 | LOSS: 0.0456 | Q_MAX: 0.0009\n",
            "EPOCH: 002/049 | WIN: 001 | LOSS: 0.0500 | Q_MAX: 0.0006\n",
            "EPOCH: 003/049 | WIN: 001 | LOSS: 0.0342 | Q_MAX: 0.0005\n",
            "EPOCH: 004/049 | WIN: 001 | LOSS: 0.0490 | Q_MAX: -0.0001\n",
            "EPOCH: 005/049 | WIN: 002 | LOSS: 0.0371 | Q_MAX: -0.0002\n",
            "EPOCH: 006/049 | WIN: 002 | LOSS: 0.0575 | Q_MAX: 0.0012\n",
            "EPOCH: 007/049 | WIN: 002 | LOSS: 0.0532 | Q_MAX: 0.0021\n",
            "EPOCH: 008/049 | WIN: 002 | LOSS: 0.0649 | Q_MAX: 0.0015\n",
            "EPOCH: 009/049 | WIN: 002 | LOSS: 0.0543 | Q_MAX: -0.0003\n",
            "EPOCH: 010/049 | WIN: 002 | LOSS: 0.0496 | Q_MAX: -0.0013\n",
            "EPOCH: 011/049 | WIN: 002 | LOSS: 0.0399 | Q_MAX: -0.0043\n",
            "EPOCH: 012/049 | WIN: 002 | LOSS: 0.0482 | Q_MAX: -0.0090\n",
            "EPOCH: 013/049 | WIN: 002 | LOSS: 0.0511 | Q_MAX: -0.0150\n",
            "EPOCH: 014/049 | WIN: 003 | LOSS: 0.0485 | Q_MAX: -0.0182\n",
            "EPOCH: 015/049 | WIN: 003 | LOSS: 0.0492 | Q_MAX: -0.0220\n",
            "EPOCH: 016/049 | WIN: 004 | LOSS: 0.0484 | Q_MAX: -0.0244\n",
            "EPOCH: 017/049 | WIN: 004 | LOSS: 0.0514 | Q_MAX: -0.0240\n",
            "EPOCH: 018/049 | WIN: 004 | LOSS: 0.0456 | Q_MAX: -0.0276\n",
            "EPOCH: 019/049 | WIN: 005 | LOSS: 0.0463 | Q_MAX: -0.0324\n",
            "EPOCH: 020/049 | WIN: 005 | LOSS: 0.0453 | Q_MAX: -0.0408\n",
            "EPOCH: 021/049 | WIN: 005 | LOSS: 0.0429 | Q_MAX: -0.0525\n",
            "EPOCH: 022/049 | WIN: 005 | LOSS: 0.0508 | Q_MAX: -0.0705\n",
            "EPOCH: 023/049 | WIN: 006 | LOSS: 0.0462 | Q_MAX: -0.0938\n",
            "EPOCH: 024/049 | WIN: 006 | LOSS: 0.0400 | Q_MAX: -0.1052\n",
            "EPOCH: 025/049 | WIN: 007 | LOSS: 0.0368 | Q_MAX: -0.1226\n",
            "EPOCH: 026/049 | WIN: 007 | LOSS: 0.0309 | Q_MAX: -0.1203\n",
            "EPOCH: 027/049 | WIN: 007 | LOSS: 0.0328 | Q_MAX: -0.1240\n",
            "EPOCH: 028/049 | WIN: 007 | LOSS: 0.0422 | Q_MAX: -0.1130\n",
            "EPOCH: 029/049 | WIN: 007 | LOSS: 0.0396 | Q_MAX: -0.1090\n",
            "EPOCH: 030/049 | WIN: 007 | LOSS: 0.0372 | Q_MAX: -0.1403\n",
            "EPOCH: 031/049 | WIN: 007 | LOSS: 0.0356 | Q_MAX: -0.1571\n",
            "EPOCH: 032/049 | WIN: 007 | LOSS: 0.0358 | Q_MAX: -0.1346\n",
            "EPOCH: 033/049 | WIN: 007 | LOSS: 0.0268 | Q_MAX: -0.1633\n",
            "EPOCH: 034/049 | WIN: 007 | LOSS: 0.0390 | Q_MAX: -0.1127\n",
            "EPOCH: 035/049 | WIN: 008 | LOSS: 0.0264 | Q_MAX: -0.0963\n",
            "EPOCH: 036/049 | WIN: 008 | LOSS: 0.0220 | Q_MAX: -0.1436\n",
            "EPOCH: 037/049 | WIN: 008 | LOSS: 0.0361 | Q_MAX: -0.2290\n",
            "EPOCH: 038/049 | WIN: 009 | LOSS: 0.0365 | Q_MAX: -0.1519\n",
            "EPOCH: 039/049 | WIN: 010 | LOSS: 0.0242 | Q_MAX: -0.1015\n",
            "EPOCH: 040/049 | WIN: 011 | LOSS: 0.0312 | Q_MAX: -0.0891\n",
            "EPOCH: 041/049 | WIN: 011 | LOSS: 0.0286 | Q_MAX: -0.2171\n",
            "EPOCH: 042/049 | WIN: 012 | LOSS: 0.0276 | Q_MAX: -0.0227\n",
            "EPOCH: 043/049 | WIN: 012 | LOSS: 0.0270 | Q_MAX: -0.1965\n",
            "EPOCH: 044/049 | WIN: 012 | LOSS: 0.0176 | Q_MAX: -0.2462\n",
            "EPOCH: 045/049 | WIN: 013 | LOSS: 0.0170 | Q_MAX: -0.1162\n",
            "EPOCH: 046/049 | WIN: 014 | LOSS: 0.0274 | Q_MAX: -0.0415\n",
            "EPOCH: 047/049 | WIN: 015 | LOSS: 0.0154 | Q_MAX: -0.1617\n",
            "EPOCH: 048/049 | WIN: 016 | LOSS: 0.0206 | Q_MAX: 0.0037\n",
            "EPOCH: 049/049 | WIN: 016 | LOSS: 0.0292 | Q_MAX: -0.1561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejxGZS41CPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "40eb57e2-7a79-4b42-b420-052698db7ce0"
      },
      "source": [
        "def init():\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "\n",
        "def animate(step):\n",
        "    global win, lose\n",
        "    global state_t_1, reward_t, terminal\n",
        "\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "\n",
        "        # for log\n",
        "        if reward_t == 1:\n",
        "            win += 1\n",
        "        elif reward_t == -1:\n",
        "            lose += 1\n",
        "\n",
        "        print(\"WIN: {:03d}/{:03d} ({:.1f}%)\".format(win, win + lose, 100 * win / (win + lose)))\n",
        "\n",
        "    else:\n",
        "        state_t = state_t_1\n",
        "\n",
        "        # execute action in environment\n",
        "        action_t = agent.select_action(state_t, 0.0)\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "    # observe environment\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "    # animate\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "env = CatchBall()\n",
        "# variables\n",
        "win, lose = 0, 0\n",
        "state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "# animate\n",
        "fig = plt.figure(figsize=(env.screen_n_rows / 2, env.screen_n_cols / 2))\n",
        "fig.canvas.set_window_title(\"{}-{}\".format(env.name, agent.name))\n",
        "img = plt.imshow(state_t_1, interpolation=\"none\", cmap=\"gray\")\n",
        "ani = animation.FuncAnimation(fig, animate, init_func=init, interval=(1000 / env.frame_rate), blit=True)\n",
        "\n",
        "w = animation.PillowWriter(fps=20)\n",
        "ani.save('animation_test.gif', writer=w)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WIN: 001/001 (100.0%)\n",
            "WIN: 001/002 (50.0%)\n",
            "WIN: 001/003 (33.3%)\n",
            "WIN: 002/004 (50.0%)\n",
            "WIN: 002/005 (40.0%)\n",
            "WIN: 003/006 (50.0%)\n",
            "WIN: 004/007 (57.1%)\n",
            "WIN: 005/008 (62.5%)\n",
            "WIN: 005/009 (55.6%)\n",
            "WIN: 006/010 (60.0%)\n",
            "WIN: 006/011 (54.5%)\n",
            "WIN: 006/012 (50.0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC7ElEQVR4nO3dMYrDQBAAwZtD///yOHJgMA7MnbcxVaEU7CTNgALt7O4P0PN7egDgOXFClDghSpwQJU6Iul69nBmfcuGf7e48e25zQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6Iuk4PcLe7HztrZj52FrzL5oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUJU5joGVyTAI5sTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdZ0egL+1u6dH+Aozc3oEmxOqxAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4/f90GRzQpQ4IUqcECVOiBInRIkTom76gBHS0GlIqAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7GQyPVq1CPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e431db8c-1382-4e69-dba5-87b97cdfab72"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}