{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DQNSimple.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenjiNakagawa/RL-public/blob/master/DQNSimple3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0wZ2fs1CPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "# import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CatchBall:\n",
        "    def __init__(self):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Env\"\n",
        "        self.screen_n_rows = 5\n",
        "        self.screen_n_cols = 10\n",
        "        self.player_length = 1\n",
        "        self.enable_actions = (0, 1, 2,3)\n",
        "        self.frame_rate = 100\n",
        "        self.counts = 0\n",
        "        \n",
        "        # variables\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, action):\n",
        "        \"\"\"\n",
        "        action:\n",
        "            0: move left\n",
        "            1: move right\n",
        "            2: move up\n",
        "            3: move down\n",
        "        \"\"\"\n",
        "        # update player position\n",
        "        if action == self.enable_actions[0]:\n",
        "            # move left\n",
        "            self.player_col = max(0, self.player_col - 1)\n",
        "        elif action == self.enable_actions[1]:\n",
        "            # move right\n",
        "            self.player_col = min(self.player_col + 1, self.screen_n_cols - 1)\n",
        "        elif action == self.enable_actions[2]:\n",
        "            self.player_row = max(0, self.player_row - 1)\n",
        "        elif action == self.enable_actions[3]:\n",
        "            self.player_row = min(self.player_row + 1, self.screen_n_rows - 1)      \n",
        "        else:\n",
        "            # do nothing\n",
        "            pass\n",
        "\n",
        "        # update ball position\n",
        "        self.counts += 1\n",
        "        # collision detection\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n",
        "        if  self.player_row == 2 and self.player_col == 9:\n",
        "            self.terminal = True\n",
        "            self.reward = 100 - self.counts\n",
        "  \n",
        "    def draw(self):\n",
        "        # reset screen\n",
        "        self.screen = np.zeros((self.screen_n_rows, self.screen_n_cols))\n",
        "\n",
        "        # draw player\n",
        "        self.screen[self.player_row, self.player_col] = 1\n",
        "\n",
        "        # draw ball\n",
        "        self.screen[2, 9] = 1\n",
        "\n",
        "    def observe(self):\n",
        "        self.draw()\n",
        "        return self.screen, self.reward, self.terminal\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        self.update(action)\n",
        "\n",
        "    def reset(self):\n",
        "        # reset player position\n",
        "        self.player_row = 2\n",
        "        self.player_col = 0\n",
        "\n",
        "        # reset other variables\n",
        "        self.counts = 0\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFfsW2g1CPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Multi Layer Perceptron with Experience Replay\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, enable_actions, environment_name):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Agent\"\n",
        "        self.environment_name = environment_name\n",
        "        self.enable_actions = enable_actions\n",
        "        self.n_actions = len(self.enable_actions)\n",
        "        self.minibatch_size = 32\n",
        "        self.replay_memory_size = 2000\n",
        "        self.learning_rate = 0.001\n",
        "        self.discount_factor = 0.99\n",
        "        self.exploration = 0.2\n",
        "        self.model_dir = \"models\"\n",
        "        self.model_name = \"{}.ckpt\".format(self.environment_name)\n",
        "\n",
        "        # replay memory\n",
        "        self.D = deque(maxlen=self.replay_memory_size)\n",
        "\n",
        "        # model\n",
        "        self.init_model()\n",
        "\n",
        "        # variables\n",
        "        self.current_loss = 0.0\n",
        "\n",
        "    def init_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "        # input layer (8 x 8)\n",
        "        self.x = tf.compat.v1.placeholder(tf.float32, [None, 5, 10])\n",
        "        \n",
        "        # flatten\n",
        "        x_flat = tf.reshape(self.x, [-1, 50])\n",
        "\n",
        "        # fully connected layer \n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([50, 50], stddev=0.01))\n",
        "        b_fc1 = tf.Variable(tf.zeros([50]))\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # output layer (n_actions)\n",
        "        W_out = tf.Variable(tf.truncated_normal([50, self.n_actions], stddev=0.01))\n",
        "        b_out = tf.Variable(tf.zeros([self.n_actions]))\n",
        "        self.y = tf.matmul(h_fc1, W_out) + b_out\n",
        "\n",
        "        # loss function\n",
        "        self.y_ = tf.compat.v1.placeholder(tf.float32, [None, self.n_actions])\n",
        "        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n",
        "\n",
        "        # train operation\n",
        "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
        "        self.training = optimizer.minimize(self.loss)\n",
        "\n",
        "        # saver\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def Q_values(self, state):\n",
        "        # Q(state, action) of all actions\n",
        "        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            # random\n",
        "            return np.random.choice(self.enable_actions)\n",
        "        else:\n",
        "            # max_action Q(state, action)\n",
        "            return self.enable_actions[np.argmax(self.Q_values(state))]\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_1, terminal):\n",
        "        self.D.append((state, action, reward, state_1, terminal))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        state_minibatch = []\n",
        "        y_minibatch = []\n",
        "\n",
        "        # sample random minibatch\n",
        "        minibatch_size = min(len(self.D), self.minibatch_size)\n",
        "        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n",
        "\n",
        "        for j in minibatch_indexes:\n",
        "            state_j, action_j, reward_j, state_j_1, terminal = self.D[j]\n",
        "            action_j_index = self.enable_actions.index(action_j)\n",
        "\n",
        "            y_j = self.Q_values(state_j)\n",
        "\n",
        "            if terminal:\n",
        "                y_j[action_j_index] = reward_j\n",
        "            else:\n",
        "                # reward_j + gamma * max_action' Q(state', action')\n",
        "                y_j[action_j_index] = reward_j + self.discount_factor * np.max(self.Q_values(state_j_1))  # NOQA\n",
        "\n",
        "            state_minibatch.append(state_j)\n",
        "            y_minibatch.append(y_j)\n",
        "\n",
        "        # training\n",
        "        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "        # for log\n",
        "        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        if model_path:\n",
        "            # load from model_path\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "        else:\n",
        "            # load from checkpoint\n",
        "            checkpoint = tf.train.get_checkpoint_state(self.model_dir)\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, os.path.join(self.model_dir, self.model_name))\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-MMHxx1CPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3291db23-d556-44d2-bfb3-0c133e19acd4"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # parameters\n",
        "    n_epochs = 20\n",
        "\n",
        "    # environment, agent\n",
        "    env = CatchBall()\n",
        "    agent = DQNAgent(env.enable_actions, env.name)\n",
        "\n",
        "    # variables\n",
        "    win = 0\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # reset\n",
        "        frame = 0\n",
        "        loss = 0.0\n",
        "        Q_max = 0.0\n",
        "        env.reset()\n",
        "        state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "        while not terminal:\n",
        "            state_t = state_t_1\n",
        "\n",
        "            # execute action in environment\n",
        "            action_t = agent.select_action(state_t, agent.exploration)\n",
        "            env.execute_action(action_t)\n",
        "\n",
        "            # observe environment\n",
        "            state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "            # store experience\n",
        "            agent.store_experience(state_t, action_t, reward_t, state_t_1, terminal)\n",
        "\n",
        "            # experience replay\n",
        "            agent.experience_replay()\n",
        "\n",
        "            # for log\n",
        "            frame += 1\n",
        "            loss += agent.current_loss\n",
        "            Q_max += np.max(agent.Q_values(state_t))\n",
        "            # if reward_t == 1:\n",
        "            #     win += 1\n",
        "\n",
        "        print(\"EPOCH: {:03d}/{:03d} | RWD: {:03d} | LOSS: {:.4f} | Q_MAX: {:.4f}\".format(\n",
        "            e, n_epochs - 1, reward_t, loss / frame, Q_max / frame))\n",
        "\n",
        "    # save model\n",
        "    agent.save_model()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 000/019 | RWD: -672 | LOSS: 0.0000 | Q_MAX: 0.0355\n",
            "EPOCH: 001/019 | RWD: 006 | LOSS: 187.6830 | Q_MAX: 0.0553\n",
            "EPOCH: 002/019 | RWD: -483 | LOSS: 139.1500 | Q_MAX: 0.1364\n",
            "EPOCH: 003/019 | RWD: -3391 | LOSS: 33.5176 | Q_MAX: 1.0961\n",
            "EPOCH: 004/019 | RWD: -17 | LOSS: 768.2138 | Q_MAX: 0.9127\n",
            "EPOCH: 005/019 | RWD: 085 | LOSS: 0.0000 | Q_MAX: 0.8864\n",
            "EPOCH: 006/019 | RWD: 022 | LOSS: 0.0001 | Q_MAX: 0.9188\n",
            "EPOCH: 007/019 | RWD: 009 | LOSS: 1978.0613 | Q_MAX: 0.9281\n",
            "EPOCH: 008/019 | RWD: -11 | LOSS: 1619.4604 | Q_MAX: 0.8202\n",
            "EPOCH: 009/019 | RWD: -2339 | LOSS: 849.5174 | Q_MAX: 9.6335\n",
            "EPOCH: 010/019 | RWD: -128 | LOSS: 188.3931 | Q_MAX: 11.2719\n",
            "EPOCH: 011/019 | RWD: -221 | LOSS: 1071.9033 | Q_MAX: 8.2632\n",
            "EPOCH: 012/019 | RWD: 061 | LOSS: 10.2355 | Q_MAX: 7.1775\n",
            "EPOCH: 013/019 | RWD: 041 | LOSS: 725.9502 | Q_MAX: 6.9492\n",
            "EPOCH: 014/019 | RWD: 028 | LOSS: 1192.1173 | Q_MAX: 6.6995\n",
            "EPOCH: 015/019 | RWD: 016 | LOSS: 1025.3819 | Q_MAX: 6.3558\n",
            "EPOCH: 016/019 | RWD: 043 | LOSS: 1503.1286 | Q_MAX: 5.9883\n",
            "EPOCH: 017/019 | RWD: 080 | LOSS: 6.8965 | Q_MAX: 5.7583\n",
            "EPOCH: 018/019 | RWD: 078 | LOSS: 4.3867 | Q_MAX: 5.7591\n",
            "EPOCH: 019/019 | RWD: 064 | LOSS: 32.0659 | Q_MAX: 5.9540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejxGZS41CPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2123c6e8-aa2d-47c6-bb5b-9c93e0673086"
      },
      "source": [
        "def init():\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "\n",
        "def animate(step):\n",
        "    global win, lose\n",
        "    global state_t_1, reward_t, terminal\n",
        "\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "        print(\"RWD:\",reward_t)\n",
        "        # print(\"WIN: {:03d}/{:03d} ({:.1f}%)\".format(win, win + lose, 100 * win / (win + lose)))\n",
        "\n",
        "    else:\n",
        "        state_t = state_t_1\n",
        "\n",
        "        # execute action in environment\n",
        "        action_t = agent.select_action(state_t, 0.0)\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "    # observe environment\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "    # animate\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "env = CatchBall()\n",
        "# variables\n",
        "win, lose = 0, 0\n",
        "state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "# animate\n",
        "fig = plt.figure(figsize=(env.screen_n_rows / 2, env.screen_n_cols / 2))\n",
        "fig.canvas.set_window_title(\"{}-{}\".format(env.name, agent.name))\n",
        "img = plt.imshow(state_t_1, interpolation=\"none\", cmap=\"gray\")\n",
        "ani = animation.FuncAnimation(fig, animate, init_func=init, interval=(10 / env.frame_rate),frames=50, blit=True)\n",
        "\n",
        "w = animation.PillowWriter(fps=20)\n",
        "ani.save('animation_test.gif', writer=w)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAABUCAYAAACRKpXQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJElEQVR4nO3dOw7CMBBAQRZx/yubnk9AUZ5AMFMmTYqnLWLLnrXWCUrnT38Av09k5ERGTmTkREZOZOQuWy9nxv8N3rbWmkfPTTJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnKbZ2Hwn44+4tUkIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiI2cXBndmHl4s8tKz3RsmGTmRkRMZOZGRExk5kZETGTmRkRMZOZGRExk5kZETGblkF8besxT2rv7z3UwyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycycnP07RNwyyQjJzJyIiMnMnIiIycyclf5GROmo4Y1QQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 180x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}