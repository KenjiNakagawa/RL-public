{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DQNSimple.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenjiNakagawa/RL-public/blob/master/DQNSimple3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0wZ2fs1CPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "# import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CatchBall:\n",
        "    def __init__(self):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Env\"\n",
        "        self.screen_n_rows = 5\n",
        "        self.screen_n_cols = 10\n",
        "        self.player_length = 3\n",
        "        self.enable_actions = (0, 1, 2,3,4)\n",
        "        self.frame_rate = 10\n",
        "        self.counts = 0\n",
        "        \n",
        "        # variables\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, action):\n",
        "        \"\"\"\n",
        "        action:\n",
        "            0: do nothing\n",
        "            1: move left\n",
        "            2: move right\n",
        "            3: move up\n",
        "            4: move down\n",
        "        \"\"\"\n",
        "        # update player position\n",
        "        if action == self.enable_actions[1]:\n",
        "            # move left\n",
        "            self.player_col = max(0, self.player_col - 1)\n",
        "        elif action == self.enable_actions[2]:\n",
        "            # move right\n",
        "            self.player_col = min(self.player_col + 1, self.screen_n_cols - 1)\n",
        "        elif action == self.enable_actions[3]:\n",
        "            self.player_row = max(0, self.player_row - 1)\n",
        "        elif action == self.enable_actions[4]:\n",
        "            self.player_row = min(self.player_row + 1, self.screen_n_rows - 1)      \n",
        "        else:\n",
        "            # do nothing\n",
        "            pass\n",
        "\n",
        "        # update ball position\n",
        "        self.counts += 1\n",
        "        # collision detection\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n",
        "        if  self.player_row == 2 and self.player_col == 9:\n",
        "            self.terminal = True\n",
        "            self.reward = 100 - self.counts\n",
        "  \n",
        "    def draw(self):\n",
        "        # reset screen\n",
        "        self.screen = np.zeros((self.screen_n_rows, self.screen_n_cols))\n",
        "\n",
        "        # draw player\n",
        "        self.screen[self.player_row, self.player_col] = 1\n",
        "\n",
        "        # draw ball\n",
        "        self.screen[2, 9] = 1\n",
        "\n",
        "    def observe(self):\n",
        "        self.draw()\n",
        "        return self.screen, self.reward, self.terminal\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        self.update(action)\n",
        "\n",
        "    def reset(self):\n",
        "        # reset player position\n",
        "        self.player_row = 2\n",
        "        self.player_col = 0\n",
        "\n",
        "        # reset other variables\n",
        "        self.counts = 0\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFfsW2g1CPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Multi Layer Perceptron with Experience Replay\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, enable_actions, environment_name):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Agent\"\n",
        "        self.environment_name = environment_name\n",
        "        self.enable_actions = enable_actions\n",
        "        self.n_actions = len(self.enable_actions)\n",
        "        self.minibatch_size = 32\n",
        "        self.replay_memory_size = 2000\n",
        "        self.learning_rate = 0.001\n",
        "        self.discount_factor = 0.99\n",
        "        self.exploration = 0.2\n",
        "        self.model_dir = \"models\"\n",
        "        self.model_name = \"{}.ckpt\".format(self.environment_name)\n",
        "\n",
        "        # replay memory\n",
        "        self.D = deque(maxlen=self.replay_memory_size)\n",
        "\n",
        "        # model\n",
        "        self.init_model()\n",
        "\n",
        "        # variables\n",
        "        self.current_loss = 0.0\n",
        "\n",
        "    def init_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "        # input layer (8 x 8)\n",
        "        self.x = tf.compat.v1.placeholder(tf.float32, [None, 5, 10])\n",
        "        \n",
        "        # flatten\n",
        "        x_flat = tf.reshape(self.x, [-1, 50])\n",
        "\n",
        "        # fully connected layer \n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([50, 50], stddev=0.01))\n",
        "        b_fc1 = tf.Variable(tf.zeros([50]))\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # output layer (n_actions)\n",
        "        W_out = tf.Variable(tf.truncated_normal([50, self.n_actions], stddev=0.01))\n",
        "        b_out = tf.Variable(tf.zeros([self.n_actions]))\n",
        "        self.y = tf.matmul(h_fc1, W_out) + b_out\n",
        "\n",
        "        # loss function\n",
        "        self.y_ = tf.compat.v1.placeholder(tf.float32, [None, self.n_actions])\n",
        "        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n",
        "\n",
        "        # train operation\n",
        "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
        "        self.training = optimizer.minimize(self.loss)\n",
        "\n",
        "        # saver\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def Q_values(self, state):\n",
        "        # Q(state, action) of all actions\n",
        "        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            # random\n",
        "            return np.random.choice(self.enable_actions)\n",
        "        else:\n",
        "            # max_action Q(state, action)\n",
        "            return self.enable_actions[np.argmax(self.Q_values(state))]\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_1, terminal):\n",
        "        self.D.append((state, action, reward, state_1, terminal))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        state_minibatch = []\n",
        "        y_minibatch = []\n",
        "\n",
        "        # sample random minibatch\n",
        "        minibatch_size = min(len(self.D), self.minibatch_size)\n",
        "        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n",
        "\n",
        "        for j in minibatch_indexes:\n",
        "            state_j, action_j, reward_j, state_j_1, terminal = self.D[j]\n",
        "            action_j_index = self.enable_actions.index(action_j)\n",
        "\n",
        "            y_j = self.Q_values(state_j)\n",
        "\n",
        "            if terminal:\n",
        "                y_j[action_j_index] = reward_j\n",
        "            else:\n",
        "                # reward_j + gamma * max_action' Q(state', action')\n",
        "                y_j[action_j_index] = reward_j + self.discount_factor * np.max(self.Q_values(state_j_1))  # NOQA\n",
        "\n",
        "            state_minibatch.append(state_j)\n",
        "            y_minibatch.append(y_j)\n",
        "\n",
        "        # training\n",
        "        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "        # for log\n",
        "        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        if model_path:\n",
        "            # load from model_path\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "        else:\n",
        "            # load from checkpoint\n",
        "            checkpoint = tf.train.get_checkpoint_state(self.model_dir)\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, os.path.join(self.model_dir, self.model_name))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-MMHxx1CPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "4d43d07f-d87c-4e6e-a735-387b5c7f32a2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # parameters\n",
        "    n_epochs = 20\n",
        "\n",
        "    # environment, agent\n",
        "    env = CatchBall()\n",
        "    agent = DQNAgent(env.enable_actions, env.name)\n",
        "\n",
        "    # variables\n",
        "    win = 0\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # reset\n",
        "        frame = 0\n",
        "        loss = 0.0\n",
        "        Q_max = 0.0\n",
        "        env.reset()\n",
        "        state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "        while not terminal:\n",
        "            state_t = state_t_1\n",
        "\n",
        "            # execute action in environment\n",
        "            action_t = agent.select_action(state_t, agent.exploration)\n",
        "            env.execute_action(action_t)\n",
        "\n",
        "            # observe environment\n",
        "            state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "            # store experience\n",
        "            agent.store_experience(state_t, action_t, reward_t, state_t_1, terminal)\n",
        "\n",
        "            # experience replay\n",
        "            agent.experience_replay()\n",
        "\n",
        "            # for log\n",
        "            frame += 1\n",
        "            loss += agent.current_loss\n",
        "            Q_max += np.max(agent.Q_values(state_t))\n",
        "            # if reward_t == 1:\n",
        "            #     win += 1\n",
        "\n",
        "        print(\"EPOCH: {:03d}/{:03d} | RWD: {:03d} | LOSS: {:.4f} | Q_MAX: {:.4f}\".format(\n",
        "            e, n_epochs - 1, reward_t, loss / frame, Q_max / frame))\n",
        "\n",
        "    # save model\n",
        "    agent.save_model()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 000/019 | RWD: -229 | LOSS: 0.0000 | Q_MAX: 0.0109\n",
            "EPOCH: 001/019 | RWD: -1642 | LOSS: 8.2554 | Q_MAX: 0.3539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7ff1b7d9c833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# experience replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# for log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-0254cbd62f3f>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0maction_j_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0my_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-0254cbd62f3f>\u001b[0m in \u001b[0;36mQ_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Q(state, action) of all actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejxGZS41CPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "e400b2f2-d70b-49de-dfc8-51541d6f5a47"
      },
      "source": [
        "def init():\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "\n",
        "def animate(step):\n",
        "    global win, lose\n",
        "    global state_t_1, reward_t, terminal\n",
        "\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "        print(\"RWD:\",reward_t)\n",
        "        # print(\"WIN: {:03d}/{:03d} ({:.1f}%)\".format(win, win + lose, 100 * win / (win + lose)))\n",
        "\n",
        "    else:\n",
        "        state_t = state_t_1\n",
        "\n",
        "        # execute action in environment\n",
        "        action_t = agent.select_action(state_t, 0.0)\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "    # observe environment\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "    # animate\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "env = CatchBall()\n",
        "# variables\n",
        "win, lose = 0, 0\n",
        "state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "# animate\n",
        "fig = plt.figure(figsize=(env.screen_n_rows / 2, env.screen_n_cols / 2))\n",
        "fig.canvas.set_window_title(\"{}-{}\".format(env.name, agent.name))\n",
        "img = plt.imshow(state_t_1, interpolation=\"none\", cmap=\"gray\")\n",
        "ani = animation.FuncAnimation(fig, animate, init_func=init, interval=(100 / env.frame_rate),frames=100, blit=True)\n",
        "\n",
        "w = animation.PillowWriter(fps=20)\n",
        "ani.save('animation_test.gif', writer=w)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAACCCAYAAABVTWriAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABjElEQVR4nO3dQQrCQBAAQUf8/5fHD5jgwVaIVccElj00cwiEmd29Qen+6wtwfSIjJzJyIiMnMnIiI/c4ezkzvm/wtt2dV89NMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycyciIjJzJyIiMnMnIiIycycqdb4rie3e8v/jPJyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycyMiJjJzIyImMnMjIiYycVYR/Zmays4/WHJpk5ERGTmTkREZOZORERk5k5ERGTmTkREZOZORERk5k5ERGTmTkREZOZORERk5k5ERGTmTkREZujn5jgk8xyciJjJzIyImMnMjIiYzcE1d2EQCcK+N0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 180x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}