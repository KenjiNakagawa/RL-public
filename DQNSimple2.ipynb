{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DQNSimple2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenjiNakagawa/RL-public/blob/master/DQNSimple2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0wZ2fs1CPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "# import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CatchBall:\n",
        "    def __init__(self):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Env\"\n",
        "        self.screen_n_rows = 20\n",
        "        self.screen_n_cols = 15\n",
        "        self.player_length = 3\n",
        "        self.enable_actions = (0, 1, 2)\n",
        "        self.frame_rate = 10\n",
        "\n",
        "        # variables\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, action):\n",
        "        \"\"\"\n",
        "        action:\n",
        "            0: do nothing\n",
        "            1: move left\n",
        "            2: move right\n",
        "        \"\"\"\n",
        "        # update player position\n",
        "        if action == self.enable_actions[1]:\n",
        "            # move left\n",
        "            self.player_col = max(0, self.player_col - 1)\n",
        "        elif action == self.enable_actions[2]:\n",
        "            # move right\n",
        "            self.player_col = min(self.player_col + 1, self.screen_n_cols - self.player_length)\n",
        "        else:\n",
        "            # do nothing\n",
        "            pass\n",
        "\n",
        "        # update ball position\n",
        "        self.ball_row += 1\n",
        "\n",
        "        # collision detection\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n",
        "        if self.ball_row == self.screen_n_rows - 1:\n",
        "            self.terminal = True\n",
        "            # self.reward = (30 - abs(self.ball_col - self.player_col) ) // 3 \n",
        "            if self.player_col <= self.ball_col < self.player_col + self.player_length:\n",
        "                # catch\n",
        "                self.reward += 1\n",
        "            else:\n",
        "                # drop\n",
        "                self.reward += -1\n",
        "\n",
        "    def draw(self):\n",
        "        # reset screen\n",
        "        self.screen = np.zeros((self.screen_n_rows, self.screen_n_cols))\n",
        "\n",
        "        # draw player\n",
        "        self.screen[self.player_row, self.player_col:self.player_col + self.player_length] = 1\n",
        "\n",
        "        # draw ball\n",
        "        self.screen[self.ball_row, self.ball_col] = 1\n",
        "\n",
        "    def observe(self):\n",
        "        self.draw()\n",
        "        return self.screen, self.reward, self.terminal\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        self.update(action)\n",
        "\n",
        "    def reset(self):\n",
        "        # reset player position\n",
        "        self.player_row = self.screen_n_rows - 1\n",
        "        self.player_col = np.random.randint(self.screen_n_cols - self.player_length)\n",
        "\n",
        "        # reset ball position\n",
        "        self.ball_row = 0\n",
        "        self.ball_col = np.random.randint(self.screen_n_cols)\n",
        "\n",
        "        # reset other variables\n",
        "        self.reward = 0\n",
        "        self.terminal = False\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFfsW2g1CPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Multi Layer Perceptron with Experience Replay\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, enable_actions, environment_name):\n",
        "        # parameters\n",
        "#         self.name = os.path.splitext(os.path.basename(__file__))[0]\n",
        "        self.name = \"Agent\"\n",
        "        self.environment_name = environment_name\n",
        "        self.enable_actions = enable_actions\n",
        "        self.n_actions = len(self.enable_actions)\n",
        "        self.minibatch_size = 32\n",
        "        self.replay_memory_size = 2000\n",
        "        self.learning_rate = 0.001\n",
        "        self.discount_factor = 0.99\n",
        "        self.exploration = 0.2\n",
        "        self.model_dir = \"models\"\n",
        "        self.model_name = \"{}.ckpt\".format(self.environment_name)\n",
        "\n",
        "        # replay memory\n",
        "        self.D = deque(maxlen=self.replay_memory_size)\n",
        "\n",
        "        # model\n",
        "        self.init_model()\n",
        "\n",
        "        # variables\n",
        "        self.current_loss = 0.0\n",
        "\n",
        "    def init_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "        # input layer (8 x 8)\n",
        "        self.x = tf.compat.v1.placeholder(tf.float32, [None, 20, 15])\n",
        "        \n",
        "        # flatten\n",
        "        x_flat = tf.reshape(self.x, [-1, 300])\n",
        "\n",
        "        # fully connected layer \n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([300, 300], stddev=0.01))\n",
        "        b_fc1 = tf.Variable(tf.zeros([300]))\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # output layer (n_actions)\n",
        "        W_out = tf.Variable(tf.truncated_normal([300, self.n_actions], stddev=0.01))\n",
        "        b_out = tf.Variable(tf.zeros([self.n_actions]))\n",
        "        self.y = tf.matmul(h_fc1, W_out) + b_out\n",
        "\n",
        "        # loss function\n",
        "        self.y_ = tf.compat.v1.placeholder(tf.float32, [None, self.n_actions])\n",
        "        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n",
        "\n",
        "        # train operation\n",
        "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
        "        self.training = optimizer.minimize(self.loss)\n",
        "\n",
        "        # saver\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def Q_values(self, state):\n",
        "        # Q(state, action) of all actions\n",
        "        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            # random\n",
        "            return np.random.choice(self.enable_actions)\n",
        "        else:\n",
        "            # max_action Q(state, action)\n",
        "            return self.enable_actions[np.argmax(self.Q_values(state))]\n",
        "\n",
        "    def store_experience(self, state, action, reward, state_1, terminal):\n",
        "        self.D.append((state, action, reward, state_1, terminal))\n",
        "\n",
        "    def experience_replay(self):\n",
        "        state_minibatch = []\n",
        "        y_minibatch = []\n",
        "\n",
        "        # sample random minibatch\n",
        "        minibatch_size = min(len(self.D), self.minibatch_size)\n",
        "        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n",
        "\n",
        "        for j in minibatch_indexes:\n",
        "            state_j, action_j, reward_j, state_j_1, terminal = self.D[j]\n",
        "            action_j_index = self.enable_actions.index(action_j)\n",
        "\n",
        "            y_j = self.Q_values(state_j)\n",
        "\n",
        "            if terminal:\n",
        "                y_j[action_j_index] = reward_j\n",
        "            else:\n",
        "                # reward_j + gamma * max_action' Q(state', action')\n",
        "                y_j[action_j_index] = reward_j + self.discount_factor * np.max(self.Q_values(state_j_1))  # NOQA\n",
        "\n",
        "            state_minibatch.append(state_j)\n",
        "            y_minibatch.append(y_j)\n",
        "\n",
        "        # training\n",
        "        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "        # for log\n",
        "        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        if model_path:\n",
        "            # load from model_path\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "        else:\n",
        "            # load from checkpoint\n",
        "            checkpoint = tf.train.get_checkpoint_state(self.model_dir)\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, os.path.join(self.model_dir, self.model_name))\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-MMHxx1CPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e54f57b0-8e25-4364-b36c-ab4be2d6690d"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # parameters\n",
        "    n_epochs = 200\n",
        "\n",
        "    # environment, agent\n",
        "    env = CatchBall()\n",
        "    agent = DQNAgent(env.enable_actions, env.name)\n",
        "\n",
        "    # variables\n",
        "    win = 0\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # reset\n",
        "        frame = 0\n",
        "        loss = 0.0\n",
        "        Q_max = 0.0\n",
        "        env.reset()\n",
        "        state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "        while not terminal:\n",
        "            state_t = state_t_1\n",
        "\n",
        "            # execute action in environment\n",
        "            action_t = agent.select_action(state_t, agent.exploration)\n",
        "            env.execute_action(action_t)\n",
        "\n",
        "            # observe environment\n",
        "            state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "            # store experience\n",
        "            agent.store_experience(state_t, action_t, reward_t, state_t_1, terminal)\n",
        "\n",
        "            # experience replay\n",
        "            agent.experience_replay()\n",
        "\n",
        "            # for log\n",
        "            frame += 1\n",
        "            loss += agent.current_loss\n",
        "            Q_max += np.max(agent.Q_values(state_t))\n",
        "            # if reward_t == 1:\n",
        "            #     win += 1\n",
        "\n",
        "        print(\"EPOCH: {:03d}/{:03d} | RWD: {:03d} | LOSS: {:.4f} | Q_MAX: {:.4f}\".format(\n",
        "            e, n_epochs - 1, reward_t, loss / frame, Q_max / frame))\n",
        "\n",
        "    # save model\n",
        "    agent.save_model()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 000/199 | RWD: 001 | LOSS: 0.0009 | Q_MAX: 0.0011\n",
            "EPOCH: 001/199 | RWD: -01 | LOSS: 0.0134 | Q_MAX: 0.0012\n",
            "EPOCH: 002/199 | RWD: -01 | LOSS: 0.0170 | Q_MAX: 0.0016\n",
            "EPOCH: 003/199 | RWD: 001 | LOSS: 0.0154 | Q_MAX: 0.0013\n",
            "EPOCH: 004/199 | RWD: 001 | LOSS: 0.0168 | Q_MAX: 0.0019\n",
            "EPOCH: 005/199 | RWD: -01 | LOSS: 0.0087 | Q_MAX: 0.0049\n",
            "EPOCH: 006/199 | RWD: -01 | LOSS: 0.0193 | Q_MAX: 0.0096\n",
            "EPOCH: 007/199 | RWD: -01 | LOSS: 0.0150 | Q_MAX: 0.0305\n",
            "EPOCH: 008/199 | RWD: -01 | LOSS: 0.0115 | Q_MAX: 0.0988\n",
            "EPOCH: 009/199 | RWD: -01 | LOSS: 0.0140 | Q_MAX: 0.1303\n",
            "EPOCH: 010/199 | RWD: 001 | LOSS: 0.0151 | Q_MAX: 0.2770\n",
            "EPOCH: 011/199 | RWD: -01 | LOSS: 0.0091 | Q_MAX: 0.5002\n",
            "EPOCH: 012/199 | RWD: -01 | LOSS: 0.0124 | Q_MAX: 0.3493\n",
            "EPOCH: 013/199 | RWD: -01 | LOSS: 0.0233 | Q_MAX: 0.8761\n",
            "EPOCH: 014/199 | RWD: 001 | LOSS: 0.0181 | Q_MAX: 0.4118\n",
            "EPOCH: 015/199 | RWD: -01 | LOSS: 0.0120 | Q_MAX: 0.8144\n",
            "EPOCH: 016/199 | RWD: -01 | LOSS: 0.0101 | Q_MAX: 0.8696\n",
            "EPOCH: 017/199 | RWD: -01 | LOSS: 0.0122 | Q_MAX: 0.7691\n",
            "EPOCH: 018/199 | RWD: -01 | LOSS: 0.0115 | Q_MAX: 0.9456\n",
            "EPOCH: 019/199 | RWD: -01 | LOSS: 0.0127 | Q_MAX: 0.6725\n",
            "EPOCH: 020/199 | RWD: -01 | LOSS: 0.0088 | Q_MAX: 0.6964\n",
            "EPOCH: 021/199 | RWD: -01 | LOSS: 0.0078 | Q_MAX: 0.4852\n",
            "EPOCH: 022/199 | RWD: 001 | LOSS: 0.0072 | Q_MAX: 0.7664\n",
            "EPOCH: 023/199 | RWD: -01 | LOSS: 0.0090 | Q_MAX: 0.1566\n",
            "EPOCH: 024/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: 0.2206\n",
            "EPOCH: 025/199 | RWD: 001 | LOSS: 0.0060 | Q_MAX: 0.7076\n",
            "EPOCH: 026/199 | RWD: -01 | LOSS: 0.0064 | Q_MAX: 0.2829\n",
            "EPOCH: 027/199 | RWD: -01 | LOSS: 0.0069 | Q_MAX: 0.1830\n",
            "EPOCH: 028/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.1948\n",
            "EPOCH: 029/199 | RWD: -01 | LOSS: 0.0045 | Q_MAX: -0.0567\n",
            "EPOCH: 030/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 0.0138\n",
            "EPOCH: 031/199 | RWD: 001 | LOSS: 0.0053 | Q_MAX: 0.2724\n",
            "EPOCH: 032/199 | RWD: -01 | LOSS: 0.0061 | Q_MAX: 0.1987\n",
            "EPOCH: 033/199 | RWD: -01 | LOSS: 0.0069 | Q_MAX: 0.3241\n",
            "EPOCH: 034/199 | RWD: -01 | LOSS: 0.0046 | Q_MAX: -0.0590\n",
            "EPOCH: 035/199 | RWD: -01 | LOSS: 0.0052 | Q_MAX: 0.2813\n",
            "EPOCH: 036/199 | RWD: -01 | LOSS: 0.0045 | Q_MAX: 0.3304\n",
            "EPOCH: 037/199 | RWD: -01 | LOSS: 0.0049 | Q_MAX: 0.1062\n",
            "EPOCH: 038/199 | RWD: -01 | LOSS: 0.0046 | Q_MAX: 0.1827\n",
            "EPOCH: 039/199 | RWD: 001 | LOSS: 0.0054 | Q_MAX: 0.1567\n",
            "EPOCH: 040/199 | RWD: 001 | LOSS: 0.0047 | Q_MAX: 0.2546\n",
            "EPOCH: 041/199 | RWD: -01 | LOSS: 0.0060 | Q_MAX: 0.0064\n",
            "EPOCH: 042/199 | RWD: -01 | LOSS: 0.0059 | Q_MAX: 0.1774\n",
            "EPOCH: 043/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: 0.3640\n",
            "EPOCH: 044/199 | RWD: 001 | LOSS: 0.0058 | Q_MAX: 0.5334\n",
            "EPOCH: 045/199 | RWD: -01 | LOSS: 0.0093 | Q_MAX: 0.7259\n",
            "EPOCH: 046/199 | RWD: -01 | LOSS: 0.0061 | Q_MAX: 0.2098\n",
            "EPOCH: 047/199 | RWD: -01 | LOSS: 0.0065 | Q_MAX: 0.2800\n",
            "EPOCH: 048/199 | RWD: -01 | LOSS: 0.0047 | Q_MAX: 0.1447\n",
            "EPOCH: 049/199 | RWD: 001 | LOSS: 0.0053 | Q_MAX: 0.3594\n",
            "EPOCH: 050/199 | RWD: 001 | LOSS: 0.0054 | Q_MAX: 0.6158\n",
            "EPOCH: 051/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: -0.1184\n",
            "EPOCH: 052/199 | RWD: 001 | LOSS: 0.0055 | Q_MAX: 0.4149\n",
            "EPOCH: 053/199 | RWD: -01 | LOSS: 0.0065 | Q_MAX: 0.7164\n",
            "EPOCH: 054/199 | RWD: -01 | LOSS: 0.0073 | Q_MAX: 0.0784\n",
            "EPOCH: 055/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.9199\n",
            "EPOCH: 056/199 | RWD: -01 | LOSS: 0.0079 | Q_MAX: -0.0002\n",
            "EPOCH: 057/199 | RWD: -01 | LOSS: 0.0070 | Q_MAX: 0.4290\n",
            "EPOCH: 058/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: 0.7882\n",
            "EPOCH: 059/199 | RWD: -01 | LOSS: 0.0077 | Q_MAX: 0.4618\n",
            "EPOCH: 060/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: 0.6719\n",
            "EPOCH: 061/199 | RWD: -01 | LOSS: 0.0095 | Q_MAX: -0.2648\n",
            "EPOCH: 062/199 | RWD: -01 | LOSS: 0.0039 | Q_MAX: 0.0673\n",
            "EPOCH: 063/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.0313\n",
            "EPOCH: 064/199 | RWD: -01 | LOSS: 0.0053 | Q_MAX: 0.7607\n",
            "EPOCH: 065/199 | RWD: -01 | LOSS: 0.0060 | Q_MAX: -0.1839\n",
            "EPOCH: 066/199 | RWD: 001 | LOSS: 0.0049 | Q_MAX: 0.4195\n",
            "EPOCH: 067/199 | RWD: -01 | LOSS: 0.0042 | Q_MAX: -0.3655\n",
            "EPOCH: 068/199 | RWD: -01 | LOSS: 0.0069 | Q_MAX: -0.2081\n",
            "EPOCH: 069/199 | RWD: -01 | LOSS: 0.0057 | Q_MAX: 0.2426\n",
            "EPOCH: 070/199 | RWD: -01 | LOSS: 0.0050 | Q_MAX: -0.0984\n",
            "EPOCH: 071/199 | RWD: -01 | LOSS: 0.0053 | Q_MAX: 0.5531\n",
            "EPOCH: 072/199 | RWD: -01 | LOSS: 0.0046 | Q_MAX: 0.2269\n",
            "EPOCH: 073/199 | RWD: 001 | LOSS: 0.0074 | Q_MAX: -0.0753\n",
            "EPOCH: 074/199 | RWD: -01 | LOSS: 0.0057 | Q_MAX: -0.1763\n",
            "EPOCH: 075/199 | RWD: -01 | LOSS: 0.0072 | Q_MAX: -0.0673\n",
            "EPOCH: 076/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: 0.5779\n",
            "EPOCH: 077/199 | RWD: -01 | LOSS: 0.0057 | Q_MAX: -0.2308\n",
            "EPOCH: 078/199 | RWD: 001 | LOSS: 0.0051 | Q_MAX: 0.1610\n",
            "EPOCH: 079/199 | RWD: 001 | LOSS: 0.0047 | Q_MAX: 0.2017\n",
            "EPOCH: 080/199 | RWD: -01 | LOSS: 0.0055 | Q_MAX: 1.0309\n",
            "EPOCH: 081/199 | RWD: -01 | LOSS: 0.0063 | Q_MAX: -0.3201\n",
            "EPOCH: 082/199 | RWD: -01 | LOSS: 0.0039 | Q_MAX: -0.0154\n",
            "EPOCH: 083/199 | RWD: -01 | LOSS: 0.0055 | Q_MAX: -0.1143\n",
            "EPOCH: 084/199 | RWD: -01 | LOSS: 0.0044 | Q_MAX: 0.2559\n",
            "EPOCH: 085/199 | RWD: -01 | LOSS: 0.0040 | Q_MAX: -0.3030\n",
            "EPOCH: 086/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: -0.2789\n",
            "EPOCH: 087/199 | RWD: -01 | LOSS: 0.0050 | Q_MAX: -0.1132\n",
            "EPOCH: 088/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: 0.3079\n",
            "EPOCH: 089/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.2012\n",
            "EPOCH: 090/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.0581\n",
            "EPOCH: 091/199 | RWD: -01 | LOSS: 0.0043 | Q_MAX: -0.2359\n",
            "EPOCH: 092/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: -0.4591\n",
            "EPOCH: 093/199 | RWD: -01 | LOSS: 0.0052 | Q_MAX: -0.5639\n",
            "EPOCH: 094/199 | RWD: -01 | LOSS: 0.0047 | Q_MAX: -0.3223\n",
            "EPOCH: 095/199 | RWD: -01 | LOSS: 0.0046 | Q_MAX: -0.3178\n",
            "EPOCH: 096/199 | RWD: 001 | LOSS: 0.0049 | Q_MAX: 0.9456\n",
            "EPOCH: 097/199 | RWD: -01 | LOSS: 0.0052 | Q_MAX: 0.1389\n",
            "EPOCH: 098/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: 0.3096\n",
            "EPOCH: 099/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: 0.4848\n",
            "EPOCH: 100/199 | RWD: 001 | LOSS: 0.0056 | Q_MAX: 0.7625\n",
            "EPOCH: 101/199 | RWD: 001 | LOSS: 0.0050 | Q_MAX: -0.1135\n",
            "EPOCH: 102/199 | RWD: -01 | LOSS: 0.0063 | Q_MAX: 0.3752\n",
            "EPOCH: 103/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: -0.5495\n",
            "EPOCH: 104/199 | RWD: -01 | LOSS: 0.0060 | Q_MAX: -0.2516\n",
            "EPOCH: 105/199 | RWD: 001 | LOSS: 0.0058 | Q_MAX: 0.5584\n",
            "EPOCH: 106/199 | RWD: -01 | LOSS: 0.0066 | Q_MAX: -0.2967\n",
            "EPOCH: 107/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: -0.0159\n",
            "EPOCH: 108/199 | RWD: -01 | LOSS: 0.0081 | Q_MAX: 0.0851\n",
            "EPOCH: 109/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 0.9385\n",
            "EPOCH: 110/199 | RWD: -01 | LOSS: 0.0042 | Q_MAX: 0.0307\n",
            "EPOCH: 111/199 | RWD: -01 | LOSS: 0.0049 | Q_MAX: -0.7206\n",
            "EPOCH: 112/199 | RWD: 001 | LOSS: 0.0060 | Q_MAX: 0.6828\n",
            "EPOCH: 113/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 1.1345\n",
            "EPOCH: 114/199 | RWD: -01 | LOSS: 0.0049 | Q_MAX: -0.5901\n",
            "EPOCH: 115/199 | RWD: -01 | LOSS: 0.0047 | Q_MAX: -0.4238\n",
            "EPOCH: 116/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: 0.2589\n",
            "EPOCH: 117/199 | RWD: -01 | LOSS: 0.0060 | Q_MAX: 0.4760\n",
            "EPOCH: 118/199 | RWD: -01 | LOSS: 0.0064 | Q_MAX: 0.0452\n",
            "EPOCH: 119/199 | RWD: 001 | LOSS: 0.0047 | Q_MAX: -0.5151\n",
            "EPOCH: 120/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: -0.4371\n",
            "EPOCH: 121/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: -0.7530\n",
            "EPOCH: 122/199 | RWD: -01 | LOSS: 0.0063 | Q_MAX: 0.4123\n",
            "EPOCH: 123/199 | RWD: 001 | LOSS: 0.0055 | Q_MAX: -0.0498\n",
            "EPOCH: 124/199 | RWD: -01 | LOSS: 0.0059 | Q_MAX: -0.6150\n",
            "EPOCH: 125/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.0131\n",
            "EPOCH: 126/199 | RWD: -01 | LOSS: 0.0040 | Q_MAX: -0.7423\n",
            "EPOCH: 127/199 | RWD: -01 | LOSS: 0.0049 | Q_MAX: -0.7393\n",
            "EPOCH: 128/199 | RWD: 001 | LOSS: 0.0051 | Q_MAX: 0.3623\n",
            "EPOCH: 129/199 | RWD: -01 | LOSS: 0.0050 | Q_MAX: -0.6667\n",
            "EPOCH: 130/199 | RWD: -01 | LOSS: 0.0054 | Q_MAX: 0.4551\n",
            "EPOCH: 131/199 | RWD: -01 | LOSS: 0.0059 | Q_MAX: -0.4072\n",
            "EPOCH: 132/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: -0.6526\n",
            "EPOCH: 133/199 | RWD: -01 | LOSS: 0.0068 | Q_MAX: -0.4058\n",
            "EPOCH: 134/199 | RWD: 001 | LOSS: 0.0060 | Q_MAX: 0.7379\n",
            "EPOCH: 135/199 | RWD: 001 | LOSS: 0.0049 | Q_MAX: -0.6275\n",
            "EPOCH: 136/199 | RWD: -01 | LOSS: 0.0060 | Q_MAX: -0.7268\n",
            "EPOCH: 137/199 | RWD: -01 | LOSS: 0.0047 | Q_MAX: -0.5991\n",
            "EPOCH: 138/199 | RWD: -01 | LOSS: 0.0053 | Q_MAX: -0.7149\n",
            "EPOCH: 139/199 | RWD: -01 | LOSS: 0.0063 | Q_MAX: -0.6574\n",
            "EPOCH: 140/199 | RWD: 001 | LOSS: 0.0053 | Q_MAX: -0.4950\n",
            "EPOCH: 141/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: -0.6987\n",
            "EPOCH: 142/199 | RWD: -01 | LOSS: 0.0048 | Q_MAX: -0.7016\n",
            "EPOCH: 143/199 | RWD: -01 | LOSS: 0.0052 | Q_MAX: -0.7332\n",
            "EPOCH: 144/199 | RWD: -01 | LOSS: 0.0058 | Q_MAX: -0.7602\n",
            "EPOCH: 145/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 0.8968\n",
            "EPOCH: 146/199 | RWD: -01 | LOSS: 0.0059 | Q_MAX: -0.7058\n",
            "EPOCH: 147/199 | RWD: -01 | LOSS: 0.0053 | Q_MAX: 0.1245\n",
            "EPOCH: 148/199 | RWD: 001 | LOSS: 0.0053 | Q_MAX: -0.4448\n",
            "EPOCH: 149/199 | RWD: -01 | LOSS: 0.0043 | Q_MAX: 0.0459\n",
            "EPOCH: 150/199 | RWD: 001 | LOSS: 0.0038 | Q_MAX: -0.6295\n",
            "EPOCH: 151/199 | RWD: 001 | LOSS: 0.0076 | Q_MAX: 0.8124\n",
            "EPOCH: 152/199 | RWD: 001 | LOSS: 0.0058 | Q_MAX: -0.2843\n",
            "EPOCH: 153/199 | RWD: 001 | LOSS: 0.0057 | Q_MAX: 0.4094\n",
            "EPOCH: 154/199 | RWD: 001 | LOSS: 0.0050 | Q_MAX: 0.2299\n",
            "EPOCH: 155/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: -0.0147\n",
            "EPOCH: 156/199 | RWD: -01 | LOSS: 0.0036 | Q_MAX: -0.3106\n",
            "EPOCH: 157/199 | RWD: 001 | LOSS: 0.0074 | Q_MAX: 0.6194\n",
            "EPOCH: 158/199 | RWD: -01 | LOSS: 0.0050 | Q_MAX: -0.4449\n",
            "EPOCH: 159/199 | RWD: 001 | LOSS: 0.0060 | Q_MAX: 1.2081\n",
            "EPOCH: 160/199 | RWD: -01 | LOSS: 0.0065 | Q_MAX: -0.4765\n",
            "EPOCH: 161/199 | RWD: -01 | LOSS: 0.0050 | Q_MAX: -0.4279\n",
            "EPOCH: 162/199 | RWD: 001 | LOSS: 0.0067 | Q_MAX: 1.0703\n",
            "EPOCH: 163/199 | RWD: -01 | LOSS: 0.0055 | Q_MAX: -0.4716\n",
            "EPOCH: 164/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: 0.7539\n",
            "EPOCH: 165/199 | RWD: 001 | LOSS: 0.0058 | Q_MAX: 0.7881\n",
            "EPOCH: 166/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: -0.3431\n",
            "EPOCH: 167/199 | RWD: -01 | LOSS: 0.0070 | Q_MAX: -0.1199\n",
            "EPOCH: 168/199 | RWD: -01 | LOSS: 0.0047 | Q_MAX: -0.0547\n",
            "EPOCH: 169/199 | RWD: -01 | LOSS: 0.0052 | Q_MAX: -0.1980\n",
            "EPOCH: 170/199 | RWD: -01 | LOSS: 0.0075 | Q_MAX: -0.2598\n",
            "EPOCH: 171/199 | RWD: 001 | LOSS: 0.0044 | Q_MAX: 0.6253\n",
            "EPOCH: 172/199 | RWD: -01 | LOSS: 0.0075 | Q_MAX: 0.6419\n",
            "EPOCH: 173/199 | RWD: 001 | LOSS: 0.0050 | Q_MAX: 0.8813\n",
            "EPOCH: 174/199 | RWD: 001 | LOSS: 0.0055 | Q_MAX: -0.1988\n",
            "EPOCH: 175/199 | RWD: 001 | LOSS: 0.0058 | Q_MAX: 1.0772\n",
            "EPOCH: 176/199 | RWD: -01 | LOSS: 0.0051 | Q_MAX: -0.4732\n",
            "EPOCH: 177/199 | RWD: -01 | LOSS: 0.0042 | Q_MAX: -0.4300\n",
            "EPOCH: 178/199 | RWD: -01 | LOSS: 0.0061 | Q_MAX: -0.2995\n",
            "EPOCH: 179/199 | RWD: 001 | LOSS: 0.0077 | Q_MAX: 0.7997\n",
            "EPOCH: 180/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 1.3423\n",
            "EPOCH: 181/199 | RWD: 001 | LOSS: 0.0051 | Q_MAX: 0.9844\n",
            "EPOCH: 182/199 | RWD: 001 | LOSS: 0.0053 | Q_MAX: 1.0378\n",
            "EPOCH: 183/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: 0.6953\n",
            "EPOCH: 184/199 | RWD: -01 | LOSS: 0.0059 | Q_MAX: 0.6791\n",
            "EPOCH: 185/199 | RWD: 001 | LOSS: 0.0064 | Q_MAX: 1.0228\n",
            "EPOCH: 186/199 | RWD: -01 | LOSS: 0.0043 | Q_MAX: -0.3671\n",
            "EPOCH: 187/199 | RWD: 001 | LOSS: 0.0073 | Q_MAX: 0.8131\n",
            "EPOCH: 188/199 | RWD: -01 | LOSS: 0.0056 | Q_MAX: -0.2570\n",
            "EPOCH: 189/199 | RWD: 001 | LOSS: 0.0065 | Q_MAX: -0.0855\n",
            "EPOCH: 190/199 | RWD: 001 | LOSS: 0.0052 | Q_MAX: -0.0599\n",
            "EPOCH: 191/199 | RWD: -01 | LOSS: 0.0062 | Q_MAX: -0.3387\n",
            "EPOCH: 192/199 | RWD: 001 | LOSS: 0.0057 | Q_MAX: 1.0355\n",
            "EPOCH: 193/199 | RWD: -01 | LOSS: 0.0055 | Q_MAX: -0.2518\n",
            "EPOCH: 194/199 | RWD: 001 | LOSS: 0.0051 | Q_MAX: -0.0333\n",
            "EPOCH: 195/199 | RWD: -01 | LOSS: 0.0043 | Q_MAX: 0.0112\n",
            "EPOCH: 196/199 | RWD: -01 | LOSS: 0.0064 | Q_MAX: -0.4398\n",
            "EPOCH: 197/199 | RWD: 001 | LOSS: 0.0051 | Q_MAX: 0.9450\n",
            "EPOCH: 198/199 | RWD: -01 | LOSS: 0.0069 | Q_MAX: 0.1483\n",
            "EPOCH: 199/199 | RWD: 001 | LOSS: 0.0062 | Q_MAX: 0.7558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejxGZS41CPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "3b6f0365-cc0d-4ec4-e1ec-1a5c86559f1e"
      },
      "source": [
        "def init():\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "\n",
        "def animate(step):\n",
        "    global win, lose\n",
        "    global state_t_1, reward_t, terminal\n",
        "\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "        print(\"RWD:\",reward_t)\n",
        "        # print(\"WIN: {:03d}/{:03d} ({:.1f}%)\".format(win, win + lose, 100 * win / (win + lose)))\n",
        "\n",
        "    else:\n",
        "        state_t = state_t_1\n",
        "\n",
        "        # execute action in environment\n",
        "        action_t = agent.select_action(state_t, 0.0)\n",
        "        env.execute_action(action_t)\n",
        "\n",
        "    # observe environment\n",
        "    state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "    # animate\n",
        "    img.set_array(state_t_1)\n",
        "    plt.axis(\"off\")\n",
        "    return img,\n",
        "\n",
        "env = CatchBall()\n",
        "# variables\n",
        "win, lose = 0, 0\n",
        "state_t_1, reward_t, terminal = env.observe()\n",
        "\n",
        "# animate\n",
        "fig = plt.figure(figsize=(env.screen_n_rows / 2, env.screen_n_cols / 2))\n",
        "fig.canvas.set_window_title(\"{}-{}\".format(env.name, agent.name))\n",
        "img = plt.imshow(state_t_1, interpolation=\"none\", cmap=\"gray\")\n",
        "ani = animation.FuncAnimation(fig, animate, init_func=init, interval=(100 / env.frame_rate),frames=200, blit=True)\n",
        "\n",
        "w = animation.PillowWriter(fps=20)\n",
        "ani.save('animation_test.gif', writer=w)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n",
            "RWD: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}